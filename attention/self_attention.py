

"""
自注意力:是一种机制，其核心思想是：
让序列中的每个元素与其他所有元素（包括自己）进行交互，计算注意力权重，从而聚合全局信息。

自注意力可以用单头注意力来实现，也可以用多头注意力来实现，多头为了增强表达能力。

自注意力同时具有并行计算和最短的最大路径长度这两个优势

"""

"""
在 Transformer 的正弦位置编码中：
低维度（如第 0、1 维） 使用 高频率（短波长）的正弦/余弦函数，能捕捉局部、精细的位置变化。
高维度（如第 510、511 维） 使用 低频率（长波长）的正弦/余弦函数，能捕捉全局、大尺度的位置关系。
"""

"""
位置编码：
pos 是位置索引
i 是维度索引,决定频率
d 是嵌入维度

"""

"""
1.三个矩阵投影(根据情况,可能不进行投影)

2. 投影后 shape 要求
Q 和 K 的最后一维必须相等(即dk),否则无法做QK'T
V 的最后一维可以不同(dv),它控制输出维度。
Q、K、V 的第一个维度(序列长度 n)通常相同，因为它们来自同一个输入序列。
但在某些变体(如 cross-attention)中,Q 和 K/V 的序列长度可以不同。

2. 举例(Transformer 中的常见设置)
在原始 Transformer 论文中：
d=512(输入维度)
dk = dv = 64(头维度)
多头注意力中，有 8 个头，每个头处理 64 维，拼接后回到 512 维。
此时 Q、K、V 的 shape 都是 nx64(单头),所以看起来一样，但这不是必须的。
"""