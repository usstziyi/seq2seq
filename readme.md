# Seq2Seqä¸æ³¨æ„åŠ›æœºåˆ¶å®ç°åº“

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9%2B-orange.svg)](https://pytorch.org/)

ä¸€ä¸ªå…¨é¢å®ç°æ·±åº¦å­¦ä¹ ä¸­åºåˆ—åˆ°åºåˆ—(Seq2Seq)æ¨¡å‹å’Œå¤šç§æ³¨æ„åŠ›æœºåˆ¶çš„Pythonåº“ï¼Œé€‚ç”¨äºæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚

## ğŸ“š é¡¹ç›®ç®€ä»‹

æœ¬åº“æä¾›äº†å¤šç§æ³¨æ„åŠ›æœºåˆ¶åŠå…¶åœ¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„åº”ç”¨å®ç°ï¼ŒåŒ…æ‹¬ç»å…¸çš„åŠ æ€§æ³¨æ„åŠ›ã€ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ç­‰æ ¸å¿ƒç»„ä»¶ã€‚æ‰€æœ‰å®ç°åŸºäºPyTorchæ¡†æ¶ï¼Œè®¾è®¡ç®€æ´æ˜äº†ï¼Œä¾¿äºå­¦ä¹ å’Œé›†æˆåˆ°å®é™…é¡¹ç›®ä¸­ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ attention/                  # æ³¨æ„åŠ›æœºåˆ¶æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ __init__.py             # æ³¨æ„åŠ›æ¨¡å—å¯¼å‡ºå®šä¹‰
â”‚   â”œâ”€â”€ additive_attention.py   # åŠ æ€§æ³¨æ„åŠ›å®ç°
â”‚   â”œâ”€â”€ dotproduct_attention.py # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å®ç°
â”‚   â””â”€â”€ multihead_attention.py  # å¤šå¤´æ³¨æ„åŠ›å®ç°
â”œâ”€â”€ common/                     # å…¬å…±å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py             # å…¬å…±æ¨¡å—å¯¼å‡ºå®šä¹‰
â”‚   â”œâ”€â”€ dataline.py             # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ heatmaps.py             # æ³¨æ„åŠ›çƒ­å›¾å¯è§†åŒ–å·¥å…·
â”‚   â””â”€â”€ mask_softmax.py         # æ©ç softmaxå®ç°
â”œâ”€â”€ decoder/                    # ç¼–ç å™¨-è§£ç å™¨æ¶æ„
â”‚   â”œâ”€â”€ __init__.py             # è§£ç å™¨æ¨¡å—å¯¼å‡ºå®šä¹‰
â”‚   â”œâ”€â”€ bahdanau_enc_dec.py     # Bahdanauæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨
â”‚   â””â”€â”€ seq2seq_enc_dec.py      # åŸºç¡€åºåˆ—åˆ°åºåˆ—ç¼–ç å™¨-è§£ç å™¨
â”œâ”€â”€ .gitignore                  # Gitå¿½ç•¥é…ç½®
â”œâ”€â”€ readme.md                   # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ test_additive_attention.py  # åŠ æ€§æ³¨æ„åŠ›æµ‹è¯•
â”œâ”€â”€ test_bahdanau_enc_dec.py    # Bahdanauç¼–ç å™¨-è§£ç å™¨æµ‹è¯•
â”œâ”€â”€ test_dotproduct_attention.py # ç‚¹ç§¯æ³¨æ„åŠ›æµ‹è¯•
â”œâ”€â”€ test_multihead_attention.py # å¤šå¤´æ³¨æ„åŠ›æµ‹è¯•
â””â”€â”€ test_seq2seq_enc_dec.py     # åºåˆ—åˆ°åºåˆ—æ¨¡å‹æµ‹è¯•
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### æ³¨æ„åŠ›æœºåˆ¶

- **åŠ æ€§æ³¨æ„åŠ› (AdditiveAttention)**
  - é€šè¿‡å¯å­¦ä¹ çš„æƒé‡å‚æ•°è®¡ç®—æŸ¥è¯¢ä¸é”®ä¹‹é—´çš„ç›¸å…³æ€§
  - é€‚ç”¨äºæŸ¥è¯¢å’Œé”®ç»´åº¦ä¸åŒçš„åœºæ™¯
  - è®¡ç®—å¤æ‚åº¦é«˜äºç‚¹ç§¯æ³¨æ„åŠ›

- **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (DotProductAttention)**
  - é€šè¿‡ç‚¹ç§¯æ“ä½œé«˜æ•ˆè®¡ç®—æ³¨æ„åŠ›æƒé‡
  - åŒ…å«æ³¨æ„åŠ›æ©ç æœºåˆ¶ï¼Œæ”¯æŒå¯å˜é•¿åº¦åºåˆ—å¤„ç†
  - å¯¹æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œç¼©æ”¾ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

- **å¤šå¤´æ³¨æ„åŠ› (MultiHeadAttention)**
  - å¹¶è¡Œä½¿ç”¨å¤šä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›å¤´ï¼Œæ•è·ä¸åŒå­ç©ºé—´çš„ç‰¹å¾
  - ç»“åˆæ³¨æ„åŠ›æœºåˆ¶å’Œçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
  - æ˜¯Transformeræ¶æ„çš„æ ¸å¿ƒç»„ä»¶

### ç¼–ç å™¨-è§£ç å™¨æ¶æ„

- **åŸºç¡€åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (BasicEncoderDecoder)**
  - ä¼ ç»Ÿçš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ŒåŸºäºGRUå®ç°
  - ç¼–ç å™¨å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºä¸Šä¸‹æ–‡å‘é‡
  - è§£ç å™¨åŸºäºä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆç›®æ ‡åºåˆ—

- **Bahdanauæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨ (BahdanauEncoderDecoder)**
  - ç»“åˆBahdanauæ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨-è§£ç å™¨
  - åœ¨è§£ç è¿‡ç¨‹ä¸­åŠ¨æ€å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†
  - è§£å†³äº†é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼Œæé«˜é•¿åºåˆ—ç¿»è¯‘è´¨é‡

### å¯è§†åŒ–å·¥å…·

- **çƒ­å›¾å¯è§†åŒ– (show_heatmaps)**
  - ç›´è§‚å±•ç¤ºæ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
  - æ”¯æŒè‡ªå®šä¹‰é¢œè‰²æ˜ å°„å’Œæ ‡æ³¨
  - å¸®åŠ©ç†è§£æ¨¡å‹å…³æ³¨çš„è¾“å…¥ä½ç½®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.7+
- PyTorch 1.9+
- NumPy
- Matplotlib
- d2l (ç”¨äºéƒ¨åˆ†æµ‹è¯•å‡½æ•°)

### å®‰è£…æ–¹æ³•

1. å…‹éš†ä»“åº“
   ```bash
   git clone https://github.com/yourusername/seq2seq-attention.git
   cd seq2seq-attention
   ```

2. å®‰è£…ä¾èµ–
   ```bash
   pip install torch numpy matplotlib
   ```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### 1. ä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

```python
from attention.dotproduct_attention import DotProductAttention
import torch

# åˆ›å»ºæ³¨æ„åŠ›æ¨¡å‹
attention = DotProductAttention(dropout=0.5)

# å‡†å¤‡è¾“å…¥æ•°æ®
batch_size = 2
num_queries = 4
num_kvpairs = 6
valid_lens = torch.tensor([3, 2])
queries = torch.ones((batch_size, num_queries, 10))
keys = torch.ones((batch_size, num_kvpairs, 10))
values = torch.ones((batch_size, num_kvpairs, 24))

# è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
output, attention_weights = attention(queries, keys, values, valid_lens)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # (2, 4, 24)
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")  # (2, 4, 6)
```

### 2. ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›

```python
from attention.multihead_attention import MultiHeadAttention
import torch

# åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å‹
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(
    key_size=num_hiddens, 
    query_size=num_hiddens, 
    value_size=num_hiddens, 
    num_hiddens=num_hiddens, 
    num_heads=num_heads, 
    dropout=0.5
)

# å‡†å¤‡è¾“å…¥æ•°æ®
batch_size = 2
num_queries = 4
num_kvpairs = 6
valid_lens = torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
Y = torch.ones((batch_size, num_kvpairs, num_hiddens))

# è®¡ç®—å¤šå¤´æ³¨æ„åŠ›è¾“å‡º
output = attention(X, Y, Y, valid_lens)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # (2, 4, 100)
```

### 3. ä½¿ç”¨Bahdanauæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨

```python
from decoder import BahdanauEncoderDecoder, BahdanauDecoder
from decoder import Seq2SeqEncoder
import torch

# å®šä¹‰æ¨¡å‹å‚æ•°
vocab_size = 10000
embed_size = 256
num_hiddens = 512
num_layers = 2

# åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨
encoder = Seq2SeqEncoder(
    src_vocab_size=vocab_size,
    embed_size=embed_size,
    num_hiddens=num_hiddens,
    num_layers=num_layers
)

decoder = BahdanauDecoder(
    vocab_size=vocab_size,
    embed_size=embed_size,
    num_hiddens=num_hiddens,
    num_layers=num_layers
)

# åˆ›å»ºBahdanauæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
model = BahdanauEncoderDecoder(encoder, decoder)

# å‡†å¤‡è¾“å…¥æ•°æ®
batch_size = 64
src_seq_len = 10
tgt_seq_len = 12

# éšæœºç”Ÿæˆæºåºåˆ—å’Œç›®æ ‡åºåˆ—
src_tokens = torch.randint(0, vocab_size, (batch_size, src_seq_len))
tgt_tokens = torch.randint(0, vocab_size, (batch_size, tgt_seq_len))
valid_lens = torch.randint(3, src_seq_len, (batch_size,))

# å‰å‘ä¼ æ’­
outputs, state = model(src_tokens, tgt_tokens, valid_lens)
print(f"è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
```

### 4. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

```python
from common.heatmaps import show_heatmaps
import torch

# ç”Ÿæˆç¤ºä¾‹æ³¨æ„åŠ›æƒé‡
attention_weights = torch.rand((2, 3, 4, 4))  # 2ä¸ªæ ·æœ¬ï¼Œ3ä¸ªæ³¨æ„åŠ›å¤´ï¼Œ4x4æ³¨æ„åŠ›æƒé‡

# å¯è§†åŒ–æ³¨æ„åŠ›çƒ­å›¾
show_heatmaps(
    attention_weights, 
    xlabel='Keys', 
    ylabel='Queries',
    titles=['Head 1', 'Head 2', 'Head 3']
)
```

## ğŸ§ª è¿è¡Œæµ‹è¯•

æœ¬é¡¹ç›®åŒ…å«å¤šä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œç”¨äºéªŒè¯å„ç»„ä»¶çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼š

```bash
# æµ‹è¯•åŠ æ€§æ³¨æ„åŠ›
python test_additive_attention.py

# æµ‹è¯•ç‚¹ç§¯æ³¨æ„åŠ›
python test_dotproduct_attention.py

# æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
python test_multihead_attention.py

# æµ‹è¯•åºåˆ—åˆ°åºåˆ—æ¨¡å‹
python test_seq2seq_enc_dec.py

# æµ‹è¯•Bahdanauæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨
python test_bahdanau_enc_dec.py
```

## ğŸ“š ç†è®ºèƒŒæ™¯

### æ³¨æ„åŠ›æœºåˆ¶åŸç†

æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºæ—¶åŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼šé€šè¿‡æŸ¥è¯¢(Query)å’Œé”®(Key)è®¡ç®—ç›¸å…³æ€§
2. **æ³¨æ„åŠ›åˆ†æ•°å½’ä¸€åŒ–**ï¼šä½¿ç”¨softmaxå°†åˆ†æ•°è½¬æ¢ä¸ºæƒé‡
3. **åŠ æƒæ±‚å’Œ**ï¼šä½¿ç”¨æƒé‡å¯¹å€¼(Value)è¿›è¡ŒåŠ æƒæ±‚å’Œ

### å¸¸è§æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

| æ³¨æ„åŠ›ç±»å‹ | è®¡ç®—æ–¹å¼ | ä¼˜ç‚¹ | é€‚ç”¨åœºæ™¯ |
|----------|---------|------|---------|
| åŠ æ€§æ³¨æ„åŠ› | å‰é¦ˆç½‘ç»œ | é€‚ç”¨äºQå’ŒKç»´åº¦ä¸åŒ | æ—©æœŸæœºå™¨ç¿»è¯‘æ¨¡å‹ |
| ç‚¹ç§¯æ³¨æ„åŠ› | ç‚¹ç§¯è¿ç®— | è®¡ç®—æ•ˆç‡é«˜ | åºåˆ—é•¿åº¦è¾ƒçŸ­åœºæ™¯ |
| ç¼©æ”¾ç‚¹ç§¯ | ç‚¹ç§¯/âˆšd_k | ç¼“è§£æ¢¯åº¦æ¶ˆå¤± | Transformeræ¨¡å‹ |
| å¤šå¤´æ³¨æ„åŠ› | å¤šç»„ç‹¬ç«‹æ³¨æ„åŠ› | æ•è·å¤šç»´åº¦ç‰¹å¾ | å¤æ‚åºåˆ—å»ºæ¨¡ä»»åŠ¡ |

## ğŸ” æ¨¡å—å¯¼å…¥æŒ‡å—

### æ¨¡å—å¯¼å…¥æ–¹å¼

1. **ç›´æ¥å¯¼å…¥ç‰¹å®šç±»æˆ–å‡½æ•°**
   ```python
   from attention.dotproduct_attention import DotProductAttention
   from decoder import BahdanauEncoderDecoder, BasicEncoderDecoder
   from common.heatmaps import show_heatmaps
   ```

2. **å¯¼å…¥æ•´ä¸ªåŒ…**
   ```python
   import attention
   import decoder
   import common
   
   # ä½¿ç”¨æ—¶
   attention_model = attention.MultiHeadAttention(...)
   decoder_model = decoder.BahdanauDecoder(...)
   ```

### æ³¨æ„äº‹é¡¹

- ç¡®ä¿é¡¹ç›®ç›®å½•åœ¨Pythonçš„æœç´¢è·¯å¾„ä¸­
- é¿å…å¾ªç¯å¯¼å…¥
- æ¨èä½¿ç”¨å…·ä½“å¯¼å…¥æ–¹å¼ï¼Œæé«˜ä»£ç å¯è¯»æ€§

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ é¸£è°¢

æœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹èµ„æºï¼š
- [d2l-ai](https://d2l.ai/) - ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹
- [PyTorchå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) è®ºæ–‡